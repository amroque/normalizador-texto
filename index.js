const fs = require('fs');
// Importamos el nuevo SDK oficial de OpenAI
const OpenAI = require('openai');

// 1. Configuramos el cliente de OpenAI
const openai = new OpenAI({
    // ¬°LA CLAVE! Apuntamos a nuestro servidor local de LM Studio
    baseURL: 'http://localhost:1234/v1',

    // Usamos una clave API ficticia. No importa lo que escribas, 
    // pero el SDK requiere que el campo exista.
    apiKey: 'not-needed-for-local'
});

// Funci√≥n principal as√≠ncrona
async function chatearConModeloLocal() {
    try {
        // 2. Leemos el prompt desde nuestro archivo de entrada
        const promptUsuario = fs.readFileSync('entrada.txt', 'utf-8');
        console.log(`üí¨ Enviando prompt: "${promptUsuario}"`);
        const meses = ['enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio', 'julio', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre'];
        const diaSemana = ['domingo', 'lunes', 'martes', 'miercoles', 'jueves', 'viernes', 'sabado'];
        // 3. ¬°LA NUEVA FORMA! Usamos el m√©todo 'chat.completions.create'
        // Esto es mucho m√°s estructurado que la llamada 'generate' de Ollama.
        const chatCompletion = await openai.chat.completions.create({
            // El formato 'messages' es el est√°ndar de OpenAI
            messages: [
                { role: 'system', content: 'Eres un capit√°n pirata rudo y hablas como tal. Siempre te quejas del clima.' },
                { role: 'user', content: promptUsuario }
            ],
            // El nombre del modelo es menos importante aqu√≠, 
            // ya que LM Studio usa el que cargaste en la UI.
            model: 'openai/gpt-oss-20b',
            temperature: 0.7, // Controla la creatividad (0.1 = preciso, 1.0 = creativo)
        });

        // 4. Extraemos y mostramos la respuesta
        const respuesta = chatCompletion.choices[0].message.content;

        console.log('ü§ñ Respuesta del Modelo:');
        console.log(respuesta);

        // 5. Guardamos la respuesta en el archivo de salida
        fs.writeFileSync('salida.txt', respuesta);
        console.log('\n‚úÖ Respuesta guardada en "salida.txt"');

    } catch (error) {
        console.error('‚ùå Ha ocurrido un error:');
        if (error.code === 'ECONNREFUSED') {
            console.error('Error: No se pudo conectar. ¬øIniciaste el servidor en LM Studio?');
        } else {
            console.error(error.message);
        }
    }
}

// Ejecutamos la funci√≥n
chatearConModeloLocal();